# reward_func 调整

## reward_func 加载过程分析

### Qwen_module reward_functions


reward函数，kwargs中包含的参数
```json

{'prompts': 
[[
    {'content': [{'type': 'text', 'text': 'You are a robot programmed for navigation tasks. You have been given a series of historical observations:'}, {'type': 'image', 'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_0.jpg'}, {'type': 'image', 'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_3.jpg'}, {'type': 'image', 'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_6.jpg'}, {'type': 'image', 'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_8.jpg'}, {'type': 'image', 'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_11.jpg'}, {'type': 'image', 'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_14.jpg'}, {'type': 'image', 'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_17.jpg'}, {'type': 'image', 'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_20.jpg'}, {'type': 'text', 'text': 'and the current observation:'}, {'type': 'image', 'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_23.jpg'}, {'type': 'text', 'text': '. Your last action is: "move forward 0.75 meters". Your assigned task is: "Face the staircase.  Walk up two sets of stairs.  Wait at the top of stairs. ". Analyze the above information and decide your next action: whether to move forward a specific distance, turn left or right by a specific angle, or stop if the task is complete.'}], 'role': 'user'}
]], 
'goal_position': [[-10.34179973602295, 3.151840925216675, 6.306970119476318]], 
'distance_to_goal': [4.722387313842773], 
'agent_position': [[-10.182180404663086, 10.207900047302246]], 
'agent_heading': [1.570796251296997], 
'image_path': [[
    '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_0.jpg', 
    '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_3.jpg', 
    '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_6.jpg', 
    '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_8.jpg', 
    '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_11.jpg', 
    '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_14.jpg', 
    '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_17.jpg', 
    '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_20.jpg', 
    '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_23.jpg'
]], 
'problem': ['You are a robot programmed for navigation tasks. You have been given a series of historical observations: <image> <image> <image> <image> <image> <image> <image> <image> and the current observation: <image>. Your last action is: "move forward 0.75 meters". Your assigned task is: "Face the staircase.  Walk up two sets of stairs.  Wait at the top of stairs. ". Analyze the above information and decide your next action: whether to move forward a specific distance, turn left or right by a specific angle, or stop if the task is complete.'], 
'accu_reward_method': ['default']
}

```




# 输入 prompt 的 tokenrize 过程调试

## QWen2.5-VL 特殊token记录

```
added_tokens_decoder={
        151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
        151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
        151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
        151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
        151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
        151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
        151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
        151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
```

## LLama-Factory tokenrize 原始数据与结果记录

在 `/home/sy1106/HDD/TZL/LLM-VLM/LLaMA-Factory/src/llamafactory/data/loader.py` 文件 `_get_preprocessed_dataset` 函数中，进行打印

```python
    print(dataset[0])
    dataset = dataset.map(
        dataset_processor.preprocess_dataset,
        batched=True,
        batch_size=data_args.preprocessing_batch_size,
        remove_columns=column_names,
        **kwargs,
    )
    print(dataset[0])
    exit()
```
第一次打印结果为：

```shell
{'_prompt': [{'content': 'You are a robot programmed for navigation tasks. You have been given current observation: <image>. Your assigned task is: "Walk towards the white bookshelf. Turn left at the white bookshelf. Stop in between the two tables. ". Analyze the above information and decide your next action: whether to move forward a specific distance, turn left or right by a specific angle, or stop if the task is complete.', 'role': 'user'}], '_response': [{'content': 'turn right 45 degrees', 'role': 'assistant'}], '_system': '', '_tools': '', '_images': ['data/vln_datasets/r2r_train_424_240/9224/9224_0.jpg'], '_videos': None, '_audios': None}
```

第二次打印结果为：

```shell
{'input_ids': [151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 2610, 525, 264, 12305, 55068, 369, 10646, 9079, 13, 1446, 614, 1012, 2661, 1482, 21930, 25, 220, 151652, 151655, 151653, 13, 4615, 12607, 3383, 374, 25, 330, 48849, 6974, 279, 4158, 2311, 53950, 13, 11999, 2115, 518, 279, 4158, 2311, 53950, 13, 14215, 304, 1948, 279, 1378, 12632, 13, 5933, 37427, 2986, 279, 3403, 1995, 323, 10279, 697, 1790, 1917, 25, 3425, 311, 3271, 4637, 264, 3151, 6010, 11, 2484, 2115, 476, 1290, 553, 264, 3151, 9210, 11, 476, 2936, 421, 279, 3383, 374, 4583, 13, 151645, 198, 151644, 77091, 198], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [412, 1290, 220, 19, 20, 12348, 151645, 198], 'images': ['data/vln_datasets/r2r_train_424_240/9224/9224_0.jpg'], 'videos': None, 'audios': None}
```

使用tokenrizer解码后为：

```shell
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
You are a robot programmed for navigation tasks. You have been given current observation: <|vision_start|><|image_pad|><|vision_end|>. Your assigned task is: "Walk towards the white bookshelf. Turn left at the white bookshelf. Stop in between the two tables. ". Analyze the above information and decide your next action: whether to move forward a specific distance, turn left or right by a specific angle, or stop if the task is complete.<|im_end|>
<|im_start|>assistant
```


### R1的错误数据记录--VLN

经过处理之后，有效内容好像都没了，应该是把分割好的13条内容全部按图像处理了，'type': 'text'的直接没了

```shell
*********************input: 
[
    {
        'goal_position': [-3.2167999744415283, 0.07484699785709381, -0.9348180294036865], 
        'distance_to_goal': 3.2942721843719482, 
        'agent_position': [-6.409084796905518, -0.12146295607089996], 
        'agent_heading': -1.5707963705062866, 
        'image_path': ['/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_0.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_3.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_9.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_15.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_17.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_21.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_26.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_30.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_32.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_35.jpg'], 
        'problem': 'You are a robot programmed for navigation tasks. You have been given a series of historical observations: <image> <image> <image> <image> <image> <image> <image> <image> <image> and the current observation: <image>. Your last action is: "move forward 0.5 meters". Your assigned task is: "Go to the left side of the dining table. Go to the small vase on the floor. ". Analyze the above information and decide your next action: whether to move forward a specific distance, turn left or right by a specific angle, or stop if the task is complete.', 'solution': 'turn left 15 degrees', 
        'accu_reward_method': 'default', 
        'prompt': [
            {
                'content': 
                [
                    {'image': None, 'text': 'You are a robot programmed for navigation tasks. You have been given a series of historical observations:', 'type': 'text'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_0.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_3.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_9.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_15.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_17.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_21.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_26.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_30.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_32.jpg', 'text': None, 'type': 'image'}, 
                    {'image': None, 'text': 'and the current observation:', 'type': 'text'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_35.jpg', 'text': None, 'type': 'image'}, 
                    {'image': None, 'text': '. Your last action is: "move forward 0.5 meters". Your assigned task is: "Go to the left side of the dining table. Go to the small vase on the floor. ". Analyze the above information and decide your next action: whether to move forward a specific distance, turn left or right by a specific angle, or stop if the task is complete.', 'type': 'text'}
                ], 
                'role': 'user'
            }
        ]
    }, 
    {
        'goal_position': [-3.2167999744415283, 0.07484699785709381, -0.9348180294036865], 
        'distance_to_goal': 3.2942721843719482, 
        'agent_position': [-6.409084796905518, -0.12146295607089996], 
        'agent_heading': -1.5707963705062866, 
        'image_path': ['/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_0.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_3.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_9.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_15.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_17.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_21.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_26.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_30.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_32.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_35.jpg'], 
        'problem': 'You are a robot programmed for navigation tasks. You have been given a series of historical observations: <image> <image> <image> <image> <image> <image> <image> <image> <image> and the current observation: <image>. Your last action is: "move forward 0.5 meters". Your assigned task is: "Go to the left side of the dining table. Go to the small vase on the floor. ". Analyze the above information and decide your next action: whether to move forward a specific distance, turn left or right by a specific angle, or stop if the task is complete.', 'solution': 'turn left 15 degrees', 
        'accu_reward_method': 'default', 
        'prompt': 
        [
            {
                'content': 
                [
                    {'image': None, 'text': 'You are a robot programmed for navigation tasks. You have been given a series of historical observations:', 'type': 'text'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_0.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_3.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_9.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_15.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_17.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_21.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_26.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_30.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_32.jpg', 'text': None, 'type': 'image'}, 
                    {'image': None, 'text': 'and the current observation:', 'type': 'text'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_35.jpg', 'text': None, 'type': 'image'}, 
                    {'image': None, 'text': '. Your last action is: "move forward 0.5 meters". Your assigned task is: "Go to the left side of the dining table. Go to the small vase on the floor. ". Analyze the above information and decide your next action: whether to move forward a specific distance, turn left or right by a specific angle, or stop if the task is complete.', 'type': 'text'}
                ], 
                'role': 'user'
            }
        ]
    }
]


*********************prompts_text: 
['<|im_start|>system\n
You are a helpful assistant.<|im_end|>\n
<|im_start|>user\n
<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|im_end|>\n
<|im_start|>assistant\n', 

'<|im_start|>system\n
You are a helpful assistant.<|im_end|>\n
<|im_start|>user\n
<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|im_end|>\n
<|im_start|>assistant\n'

]

```

## R1 coco正确数据记录

```json
[
    {
        'image_path': ['/LLM-VLM/datasets/coco/coco/train2014/COCO_train2014_000000020279.jpg'], 
        'problem': 'Please provide the bounding box coordinate of the region this sentence describes: orange.', 'solution': '<answer> [0.0, 1.12, 440.04, 255.34] </answer>', 'accu_reward_method': 'default', 
        'prompt': 
        [
            {
                'content': 
                [
                    {'text': None, 'type': 'image'}, 
                    {'text': 'Please provide the bounding box coordinate of the region this sentence describes: orange. First output the thinking process in <think> </think> tags and then output the final answer in <answer> </answer> tags. Output the final answer in JSON format.', 'type': 'text'}
                    ], 
                    'role': 'user'
            }
        ]
    }, 

    {'image_path': ['/LLM-VLM/datasets/coco/coco/train2014/COCO_train2014_000000020279.jpg'], 'problem': 'Please provide the bounding box coordinate of the region this sentence describes: orange.', 'solution': '<answer> [0.0, 1.12, 440.04, 255.34] </answer>', 'accu_reward_method': 'default', 'prompt': [{'content': [{'text': None, 'type': 'image'}, {'text': 'Please provide the bounding box coordinate of the region this sentence describes: orange. First output the thinking process in <think> </think> tags and then output the final answer in <answer> </answer> tags. Output the final answer in JSON format.', 'type': 'text'}], 'role': 'user'}]}

]
```

```json
['<|im_start|>system\n
You are a helpful assistant.<|im_end|>\n
<|im_start|>user\n
<|vision_start|><|image_pad|><|vision_end|>Please provide the bounding box coordinate of the region this sentence describes: orange. First output the thinking process in <think> </think> tags and then output the final answer in <answer> </answer> tags. Output the final answer in JSON format.<|im_end|>\n
<|im_start|>assistant\n', 

'<|im_start|>system\n
You are a helpful assistant.<|im_end|>\n
<|im_start|>user\n
<|vision_start|><|image_pad|><|vision_end|>Please provide the bounding box coordinate of the region this sentence describes: orange. First output the thinking process in <think> </think> tags and then output the final answer in <answer> </answer> tags. Output the final answer in JSON format.<|im_end|>\n<
|im_start|>assistant\n'

]
```

## 解决了，解决方案记录

### 问题来源

Hugging Face datasets.Dataset.map() 方法中无法通过参数关闭“结构对齐”机制（schema alignment）。这是 Hugging Face Datasets 的核心设计理念之一，用于保证：所有样本在相同字段上具有一致的结构和类型（schema consistency）。

Hugging Face Dataset 是 `Arrow` 表结构（columnar format）。这意味着：每一列（field）必须有相同的数据类型。每一行必须有相同字段。当你用 map() 返回某些字段缺失时，它会 补 `None` 来保持结构一致性。这是为了高效支持：多进程读取、磁盘/内存映射、快速切片和批量操作。

所以，在你处理的 `content` 列表中，每一项是 `dict`，如果你写成：

```python
{'type': 'text', 'text': 'something'}
```
但其他项是：
```python

{'type': 'image', 'image': 'xxx.jpg'}
```

那么 `HuggingFace` 会默认补 `text: None` 给 `image` 类型项，补 `image: None` 给 `text` 类型项，因为它会尝试“对齐结构”。

这将导致 `trl.data_utils.maybe_apply_chat_template` 函数错误将 文本字段解析为图片字段，从而导致不能正常生成tokenrizer所需的 prompt

### 解决方案

在应用 `trl.data_utils.maybe_apply_chat_template` 方法处理之前，首先去除 None 字段

```python
def clean_prompt(example):
    for message in example["prompt"]:
        cleaned_content = []
        for item in message["content"]:
            if item["type"] == "text":
                cleaned_content.append({
                    "type": "text",
                    "text": item["text"]
                })
            elif item["type"] == "image":
                cleaned_content.append({
                    "type": "image",
                    "image": item["image"]
                })
        message["content"] = cleaned_content
    return example

prompts_text = maybe_apply_chat_template(clean_prompt(example), processing_class)
```







logs

```
REPO_HOME: /LLM-VLM/VLM-R1
data_paths: /LLM-VLM/datasets/rl_vln/r2r_rl_train_424_240.jsonl
image_folders: /LLM-VLM/datasets/rl_vln/r2r_train_424_240
W0723 01:44:36.194000 251568 site-packages/torch/distributed/run.py:793] 
W0723 01:44:36.194000 251568 site-packages/torch/distributed/run.py:793] *****************************************
W0723 01:44:36.194000 251568 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0723 01:44:36.194000 251568 site-packages/torch/distributed/run.py:793] *****************************************
[2025-07-23 01:44:41,457] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-07-23 01:44:41,973] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-07-23 01:44:43,843] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-07-23 01:44:43,843] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
⚠️ GPU compute capability 7.5 < 8.0
🔧 Forcing attn_implementation='sdpa'
using vlm module: Qwen2VLModule
[2025-07-23 01:44:44,486] [INFO] [comm.py:652:init_distributed] cdb=None
⚠️ GPU compute capability 7.5 < 8.0
🔧 Forcing attn_implementation='sdpa'
using vlm module: Qwen2VLModule
Map (num_proc=8): 100%|███████████████████████████████████| 334259/334259 [00:43<00:00, 7711.56 examples/s]
using trainer: VLMGRPOTrainer_VLN
Map (num_proc=8): 100%|███████████████████████████████████| 334259/334259 [00:44<00:00, 7555.33 examples/s]
using trainer: VLMGRPOTrainer_VLN
Loading checkpoint shards: 100%|█████████████████████████████████████████████| 4/4 [00:02<00:00,  1.39it/s]
Applying LoRA...
Loading checkpoint shards: 100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.29it/s]
Applying LoRA...
Freezing vision modules...
Total trainable parameters: 85721088
Freezing vision modules...
Total trainable parameters: 85721088
[<function test_rewards at 0x7f48722fe9e0>]
/LLM-VLM/VLM-R1/src/open-r1-multimodal/src/open_r1/trainer/grpo_trainer_vln.py:374: UserWarning: Setting max_prompt_length is currently not supported, it has been set to None
  warnings.warn("Setting max_prompt_length is currently not supported, it has been set to None")
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
[<function test_rewards at 0x7fbc58206a70>]
/LLM-VLM/VLM-R1/src/open-r1-multimodal/src/open_r1/trainer/grpo_trainer_vln.py:374: UserWarning: Setting max_prompt_length is currently not supported, it has been set to None
  warnings.warn("Setting max_prompt_length is currently not supported, it has been set to None")
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 8. Using DeepSpeed's value.
Using /home/sy1106/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Using /home/sy1106/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/sy1106/.cache/torch_extensions/py310_cu124/fused_adam/build.ninja...
/home/sy1106/miniconda3/envs/vlm-r1/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.04516315460205078 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10246777534484863 seconds
/home/sy1106/miniconda3/envs/vlm-r1/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(


```