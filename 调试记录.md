# reward_func è°ƒæ•´

## reward_func åŠ è½½è¿‡ç¨‹åˆ†æ

### Qwen_module reward_functions


rewardå‡½æ•°ï¼Œkwargsä¸­åŒ…å«çš„å‚æ•°
```json

{'prompts': 
[[
    {'content': [{'type': 'text', 'text': 'You are a robot programmed for navigation tasks. You have been given a series of historical observations:'}, {'type': 'image', 'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_0.jpg'}, {'type': 'image', 'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_3.jpg'}, {'type': 'image', 'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_6.jpg'}, {'type': 'image', 'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_8.jpg'}, {'type': 'image', 'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_11.jpg'}, {'type': 'image', 'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_14.jpg'}, {'type': 'image', 'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_17.jpg'}, {'type': 'image', 'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_20.jpg'}, {'type': 'text', 'text': 'and the current observation:'}, {'type': 'image', 'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_23.jpg'}, {'type': 'text', 'text': '. Your last action is: "move forward 0.75 meters". Your assigned task is: "Face the staircase.  Walk up two sets of stairs.  Wait at the top of stairs. ". Analyze the above information and decide your next action: whether to move forward a specific distance, turn left or right by a specific angle, or stop if the task is complete.'}], 'role': 'user'}
]], 
'goal_position': [[-10.34179973602295, 3.151840925216675, 6.306970119476318]], 
'distance_to_goal': [4.722387313842773], 
'agent_position': [[-10.182180404663086, 10.207900047302246]], 
'agent_heading': [1.570796251296997], 
'image_path': [[
    '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_0.jpg', 
    '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_3.jpg', 
    '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_6.jpg', 
    '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_8.jpg', 
    '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_11.jpg', 
    '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_14.jpg', 
    '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_17.jpg', 
    '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_20.jpg', 
    '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/9155/9155_23.jpg'
]], 
'problem': ['You are a robot programmed for navigation tasks. You have been given a series of historical observations: <image> <image> <image> <image> <image> <image> <image> <image> and the current observation: <image>. Your last action is: "move forward 0.75 meters". Your assigned task is: "Face the staircase.  Walk up two sets of stairs.  Wait at the top of stairs. ". Analyze the above information and decide your next action: whether to move forward a specific distance, turn left or right by a specific angle, or stop if the task is complete.'], 
'accu_reward_method': ['default']
}

```




# è¾“å…¥ prompt çš„ tokenrize è¿‡ç¨‹è°ƒè¯•

## QWen2.5-VL ç‰¹æ®Štokenè®°å½•

```
added_tokens_decoder={
        151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
        151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
        151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
        151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
        151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
        151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
        151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
        151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
```

## LLama-Factory tokenrize åŸå§‹æ•°æ®ä¸ç»“æœè®°å½•

åœ¨ `/home/sy1106/HDD/TZL/LLM-VLM/LLaMA-Factory/src/llamafactory/data/loader.py` æ–‡ä»¶ `_get_preprocessed_dataset` å‡½æ•°ä¸­ï¼Œè¿›è¡Œæ‰“å°

```python
    print(dataset[0])
    dataset = dataset.map(
        dataset_processor.preprocess_dataset,
        batched=True,
        batch_size=data_args.preprocessing_batch_size,
        remove_columns=column_names,
        **kwargs,
    )
    print(dataset[0])
    exit()
```
ç¬¬ä¸€æ¬¡æ‰“å°ç»“æœä¸ºï¼š

```shell
{'_prompt': [{'content': 'You are a robot programmed for navigation tasks. You have been given current observation: <image>. Your assigned task is: "Walk towards the white bookshelf. Turn left at the white bookshelf. Stop in between the two tables. ". Analyze the above information and decide your next action: whether to move forward a specific distance, turn left or right by a specific angle, or stop if the task is complete.', 'role': 'user'}], '_response': [{'content': 'turn right 45 degrees', 'role': 'assistant'}], '_system': '', '_tools': '', '_images': ['data/vln_datasets/r2r_train_424_240/9224/9224_0.jpg'], '_videos': None, '_audios': None}
```

ç¬¬äºŒæ¬¡æ‰“å°ç»“æœä¸ºï¼š

```shell
{'input_ids': [151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 2610, 525, 264, 12305, 55068, 369, 10646, 9079, 13, 1446, 614, 1012, 2661, 1482, 21930, 25, 220, 151652, 151655, 151653, 13, 4615, 12607, 3383, 374, 25, 330, 48849, 6974, 279, 4158, 2311, 53950, 13, 11999, 2115, 518, 279, 4158, 2311, 53950, 13, 14215, 304, 1948, 279, 1378, 12632, 13, 5933, 37427, 2986, 279, 3403, 1995, 323, 10279, 697, 1790, 1917, 25, 3425, 311, 3271, 4637, 264, 3151, 6010, 11, 2484, 2115, 476, 1290, 553, 264, 3151, 9210, 11, 476, 2936, 421, 279, 3383, 374, 4583, 13, 151645, 198, 151644, 77091, 198], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [412, 1290, 220, 19, 20, 12348, 151645, 198], 'images': ['data/vln_datasets/r2r_train_424_240/9224/9224_0.jpg'], 'videos': None, 'audios': None}
```

ä½¿ç”¨tokenrizerè§£ç åä¸ºï¼š

```shell
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
You are a robot programmed for navigation tasks. You have been given current observation: <|vision_start|><|image_pad|><|vision_end|>. Your assigned task is: "Walk towards the white bookshelf. Turn left at the white bookshelf. Stop in between the two tables. ". Analyze the above information and decide your next action: whether to move forward a specific distance, turn left or right by a specific angle, or stop if the task is complete.<|im_end|>
<|im_start|>assistant
```


### R1çš„é”™è¯¯æ•°æ®è®°å½•--VLN

ç»è¿‡å¤„ç†ä¹‹åï¼Œæœ‰æ•ˆå†…å®¹å¥½åƒéƒ½æ²¡äº†ï¼Œåº”è¯¥æ˜¯æŠŠåˆ†å‰²å¥½çš„13æ¡å†…å®¹å…¨éƒ¨æŒ‰å›¾åƒå¤„ç†äº†ï¼Œ'type': 'text'çš„ç›´æ¥æ²¡äº†

```shell
*********************input: 
[
    {
        'goal_position': [-3.2167999744415283, 0.07484699785709381, -0.9348180294036865], 
        'distance_to_goal': 3.2942721843719482, 
        'agent_position': [-6.409084796905518, -0.12146295607089996], 
        'agent_heading': -1.5707963705062866, 
        'image_path': ['/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_0.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_3.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_9.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_15.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_17.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_21.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_26.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_30.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_32.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_35.jpg'], 
        'problem': 'You are a robot programmed for navigation tasks. You have been given a series of historical observations: <image> <image> <image> <image> <image> <image> <image> <image> <image> and the current observation: <image>. Your last action is: "move forward 0.5 meters". Your assigned task is: "Go to the left side of the dining table. Go to the small vase on the floor. ". Analyze the above information and decide your next action: whether to move forward a specific distance, turn left or right by a specific angle, or stop if the task is complete.', 'solution': 'turn left 15 degrees', 
        'accu_reward_method': 'default', 
        'prompt': [
            {
                'content': 
                [
                    {'image': None, 'text': 'You are a robot programmed for navigation tasks. You have been given a series of historical observations:', 'type': 'text'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_0.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_3.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_9.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_15.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_17.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_21.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_26.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_30.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_32.jpg', 'text': None, 'type': 'image'}, 
                    {'image': None, 'text': 'and the current observation:', 'type': 'text'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_35.jpg', 'text': None, 'type': 'image'}, 
                    {'image': None, 'text': '. Your last action is: "move forward 0.5 meters". Your assigned task is: "Go to the left side of the dining table. Go to the small vase on the floor. ". Analyze the above information and decide your next action: whether to move forward a specific distance, turn left or right by a specific angle, or stop if the task is complete.', 'type': 'text'}
                ], 
                'role': 'user'
            }
        ]
    }, 
    {
        'goal_position': [-3.2167999744415283, 0.07484699785709381, -0.9348180294036865], 
        'distance_to_goal': 3.2942721843719482, 
        'agent_position': [-6.409084796905518, -0.12146295607089996], 
        'agent_heading': -1.5707963705062866, 
        'image_path': ['/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_0.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_3.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_9.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_15.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_17.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_21.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_26.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_30.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_32.jpg', '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_35.jpg'], 
        'problem': 'You are a robot programmed for navigation tasks. You have been given a series of historical observations: <image> <image> <image> <image> <image> <image> <image> <image> <image> and the current observation: <image>. Your last action is: "move forward 0.5 meters". Your assigned task is: "Go to the left side of the dining table. Go to the small vase on the floor. ". Analyze the above information and decide your next action: whether to move forward a specific distance, turn left or right by a specific angle, or stop if the task is complete.', 'solution': 'turn left 15 degrees', 
        'accu_reward_method': 'default', 
        'prompt': 
        [
            {
                'content': 
                [
                    {'image': None, 'text': 'You are a robot programmed for navigation tasks. You have been given a series of historical observations:', 'type': 'text'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_0.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_3.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_9.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_15.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_17.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_21.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_26.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_30.jpg', 'text': None, 'type': 'image'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_32.jpg', 'text': None, 'type': 'image'}, 
                    {'image': None, 'text': 'and the current observation:', 'type': 'text'}, 
                    {'image': '/LLM-VLM/datasets/rl_vln/r2r_train_424_240/7369/7369_35.jpg', 'text': None, 'type': 'image'}, 
                    {'image': None, 'text': '. Your last action is: "move forward 0.5 meters". Your assigned task is: "Go to the left side of the dining table. Go to the small vase on the floor. ". Analyze the above information and decide your next action: whether to move forward a specific distance, turn left or right by a specific angle, or stop if the task is complete.', 'type': 'text'}
                ], 
                'role': 'user'
            }
        ]
    }
]


*********************prompts_text: 
['<|im_start|>system\n
You are a helpful assistant.<|im_end|>\n
<|im_start|>user\n
<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|im_end|>\n
<|im_start|>assistant\n', 

'<|im_start|>system\n
You are a helpful assistant.<|im_end|>\n
<|im_start|>user\n
<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|im_end|>\n
<|im_start|>assistant\n'

]

```

## R1 cocoæ­£ç¡®æ•°æ®è®°å½•

```json
[
    {
        'image_path': ['/LLM-VLM/datasets/coco/coco/train2014/COCO_train2014_000000020279.jpg'], 
        'problem': 'Please provide the bounding box coordinate of the region this sentence describes: orange.', 'solution': '<answer> [0.0, 1.12, 440.04, 255.34] </answer>', 'accu_reward_method': 'default', 
        'prompt': 
        [
            {
                'content': 
                [
                    {'text': None, 'type': 'image'}, 
                    {'text': 'Please provide the bounding box coordinate of the region this sentence describes: orange. First output the thinking process in <think> </think> tags and then output the final answer in <answer> </answer> tags. Output the final answer in JSON format.', 'type': 'text'}
                    ], 
                    'role': 'user'
            }
        ]
    }, 

    {'image_path': ['/LLM-VLM/datasets/coco/coco/train2014/COCO_train2014_000000020279.jpg'], 'problem': 'Please provide the bounding box coordinate of the region this sentence describes: orange.', 'solution': '<answer> [0.0, 1.12, 440.04, 255.34] </answer>', 'accu_reward_method': 'default', 'prompt': [{'content': [{'text': None, 'type': 'image'}, {'text': 'Please provide the bounding box coordinate of the region this sentence describes: orange. First output the thinking process in <think> </think> tags and then output the final answer in <answer> </answer> tags. Output the final answer in JSON format.', 'type': 'text'}], 'role': 'user'}]}

]
```

```json
['<|im_start|>system\n
You are a helpful assistant.<|im_end|>\n
<|im_start|>user\n
<|vision_start|><|image_pad|><|vision_end|>Please provide the bounding box coordinate of the region this sentence describes: orange. First output the thinking process in <think> </think> tags and then output the final answer in <answer> </answer> tags. Output the final answer in JSON format.<|im_end|>\n
<|im_start|>assistant\n', 

'<|im_start|>system\n
You are a helpful assistant.<|im_end|>\n
<|im_start|>user\n
<|vision_start|><|image_pad|><|vision_end|>Please provide the bounding box coordinate of the region this sentence describes: orange. First output the thinking process in <think> </think> tags and then output the final answer in <answer> </answer> tags. Output the final answer in JSON format.<|im_end|>\n<
|im_start|>assistant\n'

]
```

## è§£å†³äº†ï¼Œè§£å†³æ–¹æ¡ˆè®°å½•

### é—®é¢˜æ¥æº

Hugging Face datasets.Dataset.map() æ–¹æ³•ä¸­æ— æ³•é€šè¿‡å‚æ•°å…³é—­â€œç»“æ„å¯¹é½â€æœºåˆ¶ï¼ˆschema alignmentï¼‰ã€‚è¿™æ˜¯ Hugging Face Datasets çš„æ ¸å¿ƒè®¾è®¡ç†å¿µä¹‹ä¸€ï¼Œç”¨äºä¿è¯ï¼šæ‰€æœ‰æ ·æœ¬åœ¨ç›¸åŒå­—æ®µä¸Šå…·æœ‰ä¸€è‡´çš„ç»“æ„å’Œç±»å‹ï¼ˆschema consistencyï¼‰ã€‚

Hugging Face Dataset æ˜¯ `Arrow` è¡¨ç»“æ„ï¼ˆcolumnar formatï¼‰ã€‚è¿™æ„å‘³ç€ï¼šæ¯ä¸€åˆ—ï¼ˆfieldï¼‰å¿…é¡»æœ‰ç›¸åŒçš„æ•°æ®ç±»å‹ã€‚æ¯ä¸€è¡Œå¿…é¡»æœ‰ç›¸åŒå­—æ®µã€‚å½“ä½ ç”¨ map() è¿”å›æŸäº›å­—æ®µç¼ºå¤±æ—¶ï¼Œå®ƒä¼š è¡¥ `None` æ¥ä¿æŒç»“æ„ä¸€è‡´æ€§ã€‚è¿™æ˜¯ä¸ºäº†é«˜æ•ˆæ”¯æŒï¼šå¤šè¿›ç¨‹è¯»å–ã€ç£ç›˜/å†…å­˜æ˜ å°„ã€å¿«é€Ÿåˆ‡ç‰‡å’Œæ‰¹é‡æ“ä½œã€‚

æ‰€ä»¥ï¼Œåœ¨ä½ å¤„ç†çš„ `content` åˆ—è¡¨ä¸­ï¼Œæ¯ä¸€é¡¹æ˜¯ `dict`ï¼Œå¦‚æœä½ å†™æˆï¼š

```python
{'type': 'text', 'text': 'something'}
```
ä½†å…¶ä»–é¡¹æ˜¯ï¼š
```python

{'type': 'image', 'image': 'xxx.jpg'}
```

é‚£ä¹ˆ `HuggingFace` ä¼šé»˜è®¤è¡¥ `text: None` ç»™ `image` ç±»å‹é¡¹ï¼Œè¡¥ `image: None` ç»™ `text` ç±»å‹é¡¹ï¼Œå› ä¸ºå®ƒä¼šå°è¯•â€œå¯¹é½ç»“æ„â€ã€‚

è¿™å°†å¯¼è‡´ `trl.data_utils.maybe_apply_chat_template` å‡½æ•°é”™è¯¯å°† æ–‡æœ¬å­—æ®µè§£æä¸ºå›¾ç‰‡å­—æ®µï¼Œä»è€Œå¯¼è‡´ä¸èƒ½æ­£å¸¸ç”Ÿæˆtokenrizeræ‰€éœ€çš„ prompt

### è§£å†³æ–¹æ¡ˆ

åœ¨åº”ç”¨ `trl.data_utils.maybe_apply_chat_template` æ–¹æ³•å¤„ç†ä¹‹å‰ï¼Œé¦–å…ˆå»é™¤ None å­—æ®µ

```python
def clean_prompt(example):
    for message in example["prompt"]:
        cleaned_content = []
        for item in message["content"]:
            if item["type"] == "text":
                cleaned_content.append({
                    "type": "text",
                    "text": item["text"]
                })
            elif item["type"] == "image":
                cleaned_content.append({
                    "type": "image",
                    "image": item["image"]
                })
        message["content"] = cleaned_content
    return example

prompts_text = maybe_apply_chat_template(clean_prompt(example), processing_class)
```







logs

```
REPO_HOME: /LLM-VLM/VLM-R1
data_paths: /LLM-VLM/datasets/rl_vln/r2r_rl_train_424_240.jsonl
image_folders: /LLM-VLM/datasets/rl_vln/r2r_train_424_240
W0723 01:44:36.194000 251568 site-packages/torch/distributed/run.py:793] 
W0723 01:44:36.194000 251568 site-packages/torch/distributed/run.py:793] *****************************************
W0723 01:44:36.194000 251568 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0723 01:44:36.194000 251568 site-packages/torch/distributed/run.py:793] *****************************************
[2025-07-23 01:44:41,457] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-07-23 01:44:41,973] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-07-23 01:44:43,843] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-07-23 01:44:43,843] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
âš ï¸ GPU compute capability 7.5 < 8.0
ğŸ”§ Forcing attn_implementation='sdpa'
using vlm module: Qwen2VLModule
[2025-07-23 01:44:44,486] [INFO] [comm.py:652:init_distributed] cdb=None
âš ï¸ GPU compute capability 7.5 < 8.0
ğŸ”§ Forcing attn_implementation='sdpa'
using vlm module: Qwen2VLModule
Map (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334259/334259 [00:43<00:00, 7711.56 examples/s]
using trainer: VLMGRPOTrainer_VLN
Map (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334259/334259 [00:44<00:00, 7555.33 examples/s]
using trainer: VLMGRPOTrainer_VLN
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.39it/s]
Applying LoRA...
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.29it/s]
Applying LoRA...
Freezing vision modules...
Total trainable parameters: 85721088
Freezing vision modules...
Total trainable parameters: 85721088
[<function test_rewards at 0x7f48722fe9e0>]
/LLM-VLM/VLM-R1/src/open-r1-multimodal/src/open_r1/trainer/grpo_trainer_vln.py:374: UserWarning: Setting max_prompt_length is currently not supported, it has been set to None
  warnings.warn("Setting max_prompt_length is currently not supported, it has been set to None")
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
[<function test_rewards at 0x7fbc58206a70>]
/LLM-VLM/VLM-R1/src/open-r1-multimodal/src/open_r1/trainer/grpo_trainer_vln.py:374: UserWarning: Setting max_prompt_length is currently not supported, it has been set to None
  warnings.warn("Setting max_prompt_length is currently not supported, it has been set to None")
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 8. Using DeepSpeed's value.
Using /home/sy1106/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Using /home/sy1106/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/sy1106/.cache/torch_extensions/py310_cu124/fused_adam/build.ninja...
/home/sy1106/miniconda3/envs/vlm-r1/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.04516315460205078 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10246777534484863 seconds
/home/sy1106/miniconda3/envs/vlm-r1/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(


```